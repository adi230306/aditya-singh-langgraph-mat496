{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0c9e547f",
      "metadata": {
        "id": "0c9e547f"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
      "metadata": {
        "id": "319adfec-2d0a-49f2-87f9-275c4a32add2"
      },
      "source": [
        "# Streaming\n",
        "\n",
        "## Review\n",
        "\n",
        "In module 2, covered a few ways to customize graph state and memory.\n",
        "\n",
        "We built up to a Chatbot with external memory that can sustain long-running conversations.\n",
        "\n",
        "## Goals\n",
        "\n",
        "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways.\n",
        "\n",
        "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai langchain-core langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltP8yYBCI1cx",
        "outputId": "de50bcf5-229a-4854-c346-999a4acc8e23"
      },
      "id": "ltP8yYBCI1cx",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.0.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-1.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-1.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.37)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.11.10)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-1.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n",
            "Downloading langchain_openai-1.0.1-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.1-py3-none-any.whl (467 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.1/467.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-1.0.1-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.1-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-1.0.1 langchain-openai-1.0.1 langgraph-1.0.1 langgraph-checkpoint-3.0.0 langgraph-prebuilt-1.0.1 langgraph-sdk-0.2.9 ormsgpack-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
      "metadata": {
        "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langJupyter:chain_openai langgraph_sdk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
      "metadata": {
        "id": "70d7e41b-c6ba-4e47-b645-6c110bede549"
      },
      "source": [
        "## Streaming\n",
        "\n",
        "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
        "\n",
        "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5b430d92-f595-4322-a56e-06de7485daa8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b430d92-f595-4322-a56e-06de7485daa8",
        "outputId": "4474bea1-432b-45f5-9523-2bb61d19981e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0682fc",
      "metadata": {
        "id": "4d0682fc"
      },
      "source": [
        "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
        "outputId": "c5be4f79-2e01-416d-ea77-cc75bcfaadbb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Simple model\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Use basic MessagesState (no custom summary)\n",
        "class State(MessagesState):\n",
        "    pass\n",
        "\n",
        "# Define the model calling function\n",
        "def call_model(state: State):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "# Create graph\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"chat\", call_model)\n",
        "workflow.add_edge(START, \"chat\")\n",
        "workflow.add_edge(\"chat\", END)\n",
        "\n",
        "# Compile with memory\n",
        "memory = MemorySaver()\n",
        "graph = workflow.compile(checkpointer=memory)\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f847a787-b301-488c-9b58-cba9f389f55d",
      "metadata": {
        "id": "f847a787-b301-488c-9b58-cba9f389f55d"
      },
      "source": [
        "### Streaming full state\n",
        "\n",
        "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
        "\n",
        "`.stream` and `.astream` are sync and async methods for streaming back results.\n",
        "\n",
        "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
        "\n",
        "* `values`: This streams the full state of the graph after each node is called.\n",
        "* `updates`: This streams updates to the state of the graph after each node is called.\n",
        "\n",
        "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
        "\n",
        "Let's look at `stream_mode=\"updates\"`.\n",
        "\n",
        "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
        "\n",
        "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
        "outputId": "f51909f2-e2f7-4894-eb1f-8e9514fe2437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'chat': {'messages': AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 9, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CVJ0mh1awO22F3rhGLcL71ebdwKbu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--a2fd7e23-34fb-4734-afe2-2217b48abe77-0', usage_metadata={'input_tokens': 9, 'output_tokens': 9, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"simple-1\"}}\n",
        "\n",
        "for chunk in graph.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"Hello!\")]},\n",
        "    config,\n",
        "    stream_mode=\"updates\"\n",
        "):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
      "metadata": {
        "id": "0c4882e9-07dd-4d70-866b-dfc530418cad"
      },
      "source": [
        "Let's now just print the state update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c859c777-cb12-4682-9108-6b367e597b81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c859c777-cb12-4682-9108-6b367e597b81",
        "outputId": "d506c988-0c54-40c6-9a49-6f91adae6972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi there!\n",
            "----------------------------------------\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi there!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello! How can I assist you today?\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"simple-2\"}}\n",
        "\n",
        "for event in graph.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"Hi there!\")]},\n",
        "    config,\n",
        "    stream_mode=\"values\"\n",
        "):\n",
        "    for message in event['messages']:\n",
        "        message.pretty_print()\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583bf219-6358-4d06-ae99-c40f43569fda",
      "metadata": {
        "id": "583bf219-6358-4d06-ae99-c40f43569fda"
      },
      "source": [
        "Now, we can see `stream_mode=\"values\"`.\n",
        "\n",
        "This is the `full state` of the graph after the `conversation` node is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
        "outputId": "df21aee6-7f38-4fd1-bddd-8d2f69a5bbcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: \n",
            "Token: Why\n",
            "Token:  couldn\n",
            "Token: 't\n",
            "Token:  the\n",
            "Token:  bicycle\n",
            "Token:  stand\n",
            "Token:  up\n",
            "Token:  by\n",
            "Token:  itself\n",
            "Token: ?\n",
            "Token:  Because\n",
            "Token:  it\n",
            "Token:  was\n",
            "Token:  two\n",
            "Token:  tired\n",
            "Token: !\n",
            "Token: \n",
            "Token: \n",
            "Token: \n"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"simple-3\"}}\n",
        "input_message = HumanMessage(content=\"Tell me a short joke\")\n",
        "\n",
        "async for event in graph.astream_events(\n",
        "    {\"messages\": [input_message]},\n",
        "    config,\n",
        "    version=\"v2\"\n",
        "):\n",
        "    if event[\"event\"] == \"on_chat_model_stream\":\n",
        "        print(f\"Token: {event['data']['chunk'].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
      "metadata": {
        "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7"
      },
      "source": [
        "### Streaming tokens\n",
        "\n",
        "We often want to stream more than graph state.\n",
        "\n",
        "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
        "\n",
        "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
        "\n",
        "Each event is a dict with a few keys:\n",
        "\n",
        "* `event`: This is the type of event that is being emitted.\n",
        "* `name`: This is the name of event.\n",
        "* `data`: This is the data associated with the event.\n",
        "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
        "\n",
        "Let's have a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
        "outputId": "a9ed8d1b-9f35-4b23-fd98-24833aeb5bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 multiplied by 9 equals 54."
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"simple-4\"}}\n",
        "input_message = HumanMessage(content=\"What's 6*9?\")\n",
        "\n",
        "async for event in graph.astream_events(\n",
        "    {\"messages\": [input_message]},\n",
        "    config,\n",
        "    version=\"v2\"\n",
        "):\n",
        "    if event[\"event\"] == \"on_chat_model_stream\":\n",
        "        print(event['data'][\"chunk\"].content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
      "metadata": {
        "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d"
      },
      "source": [
        "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
        "\n",
        "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
        "\n",
        "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
        "outputId": "2b3921bb-504e-4258-aec6-dfe8d14d5c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' professional', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' American', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' based', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Bay', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Area', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' compete', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' League', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='NFL', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' member', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' league', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='N', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' division', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' was', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' founded', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='6', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' charter', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' member', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' All', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='-Amer', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ica', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='AA', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' joined', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' when', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' leagues', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' merged', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Key', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Points', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=':\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='St', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='adium', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' play', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' home', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' games', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Levi', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Stadium', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Santa', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Clara', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' California', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' which', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' moved', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='4', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Before', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' that', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' played', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Cand', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='lestick', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Park', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='Team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Colors', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Masc', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ot', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' colors', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' red', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' gold', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' white', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' mascot', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' S', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ourd', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ough', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Sam', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='Champ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ionship', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' five', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' championships', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' XVI', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' XIX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='III', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='IV', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='IX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='),', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' most', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' successful', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' period', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' being', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='198', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' early', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='199', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='Not', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='able', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Players', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' had', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' several', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Hall', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Fame', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' players', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Joe', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Montana', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Jerry', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Rice', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Steve', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Young', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Ronnie', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' L', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ott', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Charles', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Haley', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' among', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' others', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='Co', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='aching', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Management', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' been', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' led', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' by', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' several', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' notable', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' coaches', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Bill', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Walsh', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' who', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' credited', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' popular', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='izing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' \"', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='West', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Coast', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Off', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ense', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.\"', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' As', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' latest', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' updates', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Kyle', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Shan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ahan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' head', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' coach', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='R', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ival', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ries', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' notable', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' rival', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ries', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' teams', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' such', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Dallas', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Cowboys', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Los', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Angeles', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Rams', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Seattle', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Seahawks', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='Recent', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Performance', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' In', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' recent', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' years', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' been', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' competitive', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' reaching', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' season', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' but', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' losing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Kansas', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' City', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Chiefs', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' been', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' strong', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' defense', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' dynamic', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' offense', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' under', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' Shan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ahan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' leadership', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' rich', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' passionate', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' fan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' base', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' making', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' them', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' one', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' iconic', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' franchises', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10')}\n",
            "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10', chunk_position='last')}\n",
            "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10', usage_metadata={'input_tokens': 16, 'output_tokens': 412, 'total_tokens': 428, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}\n",
            "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--e3360363-3084-4340-9f8d-29804a932b10', chunk_position='last')}\n"
          ]
        }
      ],
      "source": [
        "node_to_stream = 'conversation'\n",
        "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
        "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "    # Get chat model tokens from a particular node\n",
        "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
        "        print(event[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
      "metadata": {
        "id": "226e569a-76c3-43d8-8f89-3ae687efde1c"
      },
      "source": [
        "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
        "outputId": "7487d62d-8704-4f11-dcee-6e037de83063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|The| San| Francisco| |49|ers| are| a| professional| American| football| team| based| in| the| San| Francisco| Bay| Area|.| They| compete| in| the| National| Football| League| (|NFL|)| as| a| member| club| of| the| league|'s| National| Football| Conference| (|N|FC|)| West| division|.| The| team| was| founded| in| |194|6| as| a| charter| member| of| the| All|-Amer|ica| Football| Conference| (|AA|FC|)| and| joined| the| NFL| in| |194|9| when| the| leagues| merged|.\n",
            "\n",
            "|###| Key| Points|:\n",
            "\n",
            "|-| **|Team| Name| and| Colors|**|:| The| team| is| named| after| the| prospect|ors| who| arrived| in| Northern| California| during| the| |184|9| Gold| Rush|.| The| team's| colors| are| red|,| gold|,| and| white|.\n",
            "\n",
            "|-| **|St|adium|**|:| The| |49|ers| play| their| home| games| at| Levi|'s| Stadium| in| Santa| Clara|,| California|,| which| they| moved| to| in| |201|4|.| Before| that|,| they| played| at| Cand|lestick| Park| in| San| Francisco|.\n",
            "\n",
            "|-| **|Champ|ionship|s|**|:| The| |49|ers| have| won| five| Super| Bowl| titles| (|X|VI|,| XIX|,| XX|III|,| XX|IV|,| and| XX|IX|),| with| their| most| successful| period| being| the| |198|0|s| and| early| |199|0|s|.| They| have| also| won| numerous| division| titles| and| conference| championships|.\n",
            "\n",
            "|-| **|Not|able| Figures|**|:| The| team| has| had| several| Hall| of| Fame| players|,| including| Joe| Montana|,| Jerry| Rice|,| Steve| Young|,| Ronnie| L|ott|,| and| Charles| Haley|.| Bill| Walsh|,| the| legendary| head| coach|,| is| credited| with| developing| the| West| Coast| offense|,| which| became| a| staple| of| the| team's| success|.\n",
            "\n",
            "|-| **|R|ival|ries|**|:| The| |49|ers| have| notable| rival|ries| with| the| Seattle| Seahawks|,| Los| Angeles| Rams|,| and| historically| with| the| Dallas| Cowboys| and| Green| Bay| Packers|.\n",
            "\n",
            "|-| **|Recent| Performance|**|:| In| recent| years|,| the| |49|ers| have| been| competitive|,| reaching| the| Super| Bowl| in| the| |201|9| season| but| losing| to| the| Kansas| City| Chiefs|.| They| have| been| known| for| their| strong| defense| and| innovative| offensive| strategies| under| head| coach| Kyle| Shan|ahan|.\n",
            "\n",
            "|The| |49|ers| have| a| rich| history| and| a| passionate| fan| base|,| making| them| one| of| the| iconic| franchises| in| the| NFL|.||||"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
        "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "    # Get chat model tokens from a particular node\n",
        "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
        "        data = event[\"data\"]\n",
        "        print(data[\"chunk\"].content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
      "metadata": {
        "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db"
      },
      "source": [
        "### Streaming with LangGraph API\n",
        "\n",
        "**⚠️ DISCLAIMER**\n",
        "\n",
        "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
        "\n",
        "```\n",
        "langgraph dev\n",
        "```\n",
        "\n",
        "You should see the following output:\n",
        "```\n",
        "- 🚀 API: http://127.0.0.1:2024\n",
        "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
        "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
        "```\n",
        "\n",
        "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
        "\n",
        "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
        "outputId": "0bb892ec-bffe-49cb-8cfa-ea6d50eaacc4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Unfortunately LangGraph Studio is currently not supported on Google Colab",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3613483498.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'google.colab'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unfortunately LangGraph Studio is currently not supported on Google Colab\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: Unfortunately LangGraph Studio is currently not supported on Google Colab"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "079c2ad6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "079c2ad6",
        "outputId": "64fe6557-e9f0-4150-e137-f7728ed337d3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectError",
          "evalue": "All connection attempts failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection_pool.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = await connection.handle_async_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect_failed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_async/connection.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connect_tcp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_tcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/auto.py\u001b[0m in \u001b[0;36mconnect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         return await self._backend.connect_tcp(\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/anyio.py\u001b[0m in \u001b[0;36mconnect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    112\u001b[0m         }\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfail_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_exceptions.py\u001b[0m in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mto_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mraise\u001b[0m  \u001b[0;31m# pragma: nocover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectError\u001b[0m: All connection attempts failed",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4227330451.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Search all hosted graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0massistants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massistants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph_sdk/client.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, metadata, graph_id, limit, offset, sort_by, sort_order, select, headers, params)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mpayload\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"select\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         return await self.http.post(\n\u001b[0m\u001b[1;32m   1125\u001b[0m             \u001b[0;34m\"/assistants/search\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph_sdk/client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, json, params, headers, on_response)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mrequest_headers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         r = await self.client.post(\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mParameters\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \"\"\"\n\u001b[0;32m-> 1859\u001b[0;31m         return await self.request(\n\u001b[0m\u001b[1;32m   1860\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1538\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         )\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0masynccontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         response = await self._send_handling_auth(\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                 response = await self._send_handling_redirects(\n\u001b[0m\u001b[1;32m   1658\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m   1692\u001b[0m                 \u001b[0;32mawait\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_async_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mmapped_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectError\u001b[0m: All connection attempts failed"
          ]
        }
      ],
      "source": [
        "from langgraph_sdk import get_client\n",
        "\n",
        "# This is the URL of the local development server\n",
        "URL = \"http://127.0.0.1:2024\"\n",
        "client = get_client(url=URL)\n",
        "\n",
        "# Search all hosted graphs\n",
        "assistants = await client.assistants.search()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
      "metadata": {
        "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32"
      },
      "source": [
        "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
      "metadata": {
        "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf"
      },
      "outputs": [],
      "source": [
        "# Create a new thread\n",
        "thread = await client.threads.create()\n",
        "# Input message\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"],\n",
        "                                      assistant_id=\"agent\",\n",
        "                                      input={\"messages\": [input_message]},\n",
        "                                      stream_mode=\"values\"):\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
      "metadata": {
        "id": "556dc7fd-1cae-404f-816a-f13d772b3b14"
      },
      "source": [
        "The streamed objects have:\n",
        "\n",
        "* `event`: Type\n",
        "* `data`: State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b735aa-139c-45a3-a850-63519c0004f0",
      "metadata": {
        "id": "57b735aa-139c-45a3-a850-63519c0004f0"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import convert_to_messages\n",
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
        "    messages = event.data.get('messages',None)\n",
        "    if messages:\n",
        "        print(convert_to_messages(messages)[-1])\n",
        "    print('='*25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a555d186-27be-4ddf-934c-895a3105035d",
      "metadata": {
        "id": "a555d186-27be-4ddf-934c-895a3105035d"
      },
      "source": [
        "There are some new streaming mode that are only supported via the API.\n",
        "\n",
        "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
        "\n",
        "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
        "\n",
        "All events emitted using `messages` mode have two attributes:\n",
        "\n",
        "* `event`: This is the name of the event\n",
        "* `data`: This is data associated with the event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
      "metadata": {
        "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45"
      },
      "outputs": [],
      "source": [
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"],\n",
        "                                      assistant_id=\"agent\",\n",
        "                                      input={\"messages\": [input_message]},\n",
        "                                      stream_mode=\"messages\"):\n",
        "    print(event.event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
      "metadata": {
        "id": "8de2f1ea-b232-43fc-af7a-320efce83381"
      },
      "source": [
        "We can see a few events:\n",
        "\n",
        "* `metadata`: metadata about the run\n",
        "* `messages/complete`: fully formed message\n",
        "* `messages/partial`: chat model tokens\n",
        "\n",
        "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
        "\n",
        "Now, let's show how to stream these messages.\n",
        "\n",
        "We'll define a helper function for better formatting of the tool calls in messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
      "metadata": {
        "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522"
      },
      "outputs": [],
      "source": [
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "\n",
        "def format_tool_calls(tool_calls):\n",
        "    \"\"\"\n",
        "    Format a list of tool calls into a readable string.\n",
        "\n",
        "    Args:\n",
        "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
        "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if tool_calls:\n",
        "        formatted_calls = []\n",
        "        for call in tool_calls:\n",
        "            formatted_calls.append(\n",
        "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
        "            )\n",
        "        return \"\\n\".join(formatted_calls)\n",
        "    return \"No tool calls\"\n",
        "\n",
        "async for event in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"agent\",\n",
        "    input={\"messages\": [input_message]},\n",
        "    stream_mode=\"messages\",):\n",
        "\n",
        "    # Handle metadata events\n",
        "    if event.event == \"metadata\":\n",
        "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Handle partial message events\n",
        "    elif event.event == \"messages/partial\":\n",
        "        for data_item in event.data:\n",
        "            # Process user messages\n",
        "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
        "                print(f\"Human: {data_item['content']}\")\n",
        "            else:\n",
        "                # Extract relevant data from the event\n",
        "                tool_calls = data_item.get(\"tool_calls\", [])\n",
        "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
        "                content = data_item.get(\"content\", \"\")\n",
        "                response_metadata = data_item.get(\"response_metadata\", {})\n",
        "\n",
        "                if content:\n",
        "                    print(f\"AI: {content}\")\n",
        "\n",
        "                if tool_calls:\n",
        "                    print(\"Tool Calls:\")\n",
        "                    print(format_tool_calls(tool_calls))\n",
        "\n",
        "                if invalid_tool_calls:\n",
        "                    print(\"Invalid Tool Calls:\")\n",
        "                    print(format_tool_calls(invalid_tool_calls))\n",
        "\n",
        "                if response_metadata:\n",
        "                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n",
        "                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n",
        "\n",
        "        print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
      "metadata": {
        "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}